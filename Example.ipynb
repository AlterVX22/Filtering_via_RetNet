{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27afca67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/AlterVX22/Filtering_via_RetNet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6025729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Filtering_via_RetNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35157ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torchscale.architecture.config import RetNetConfig\n",
    "from torchscale.architecture.retnet import RetNetDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5082b5b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# location of datasets: https://www.kaggle.com/datasets/vxmindset22/okko-data\n",
    "with open('datasets.pkl', 'rb') as f:\n",
    "    datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b97710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (         userId  movieId\n",
       "  1926171  185543     2016\n",
       "  1926172   50933     3284\n",
       "  1926173  230764       68\n",
       "  1926174  186260     1473\n",
       "  1926175   94969      493\n",
       "  ...         ...      ...\n",
       "  9630848  479013     3766\n",
       "  9630849   29856      876\n",
       "  9630850  277832     4771\n",
       "  9630851   77545      491\n",
       "  9630852  260632     6147\n",
       "  \n",
       "  [7704682 rows x 2 columns],\n",
       "  1926171    0.189311\n",
       "  1926172    0.390769\n",
       "  1926173    0.010635\n",
       "  1926174    0.102065\n",
       "  1926175    0.200385\n",
       "               ...   \n",
       "  9630848    0.089422\n",
       "  9630849    0.200385\n",
       "  9630850    0.176240\n",
       "  9630851    0.190627\n",
       "  9630852    0.200385\n",
       "  Name: rating_scaled, Length: 7704682, dtype: float64),\n",
       " 'test': (         userId  movieId\n",
       "  0             0        0\n",
       "  1             1        1\n",
       "  2             2        2\n",
       "  3             3        3\n",
       "  4             4        4\n",
       "  ...         ...      ...\n",
       "  1926166  236143      587\n",
       "  1926167  120603      130\n",
       "  1926168   56511      245\n",
       "  1926169  149668      493\n",
       "  1926170  215417      302\n",
       "  \n",
       "  [1926171 rows x 2 columns],\n",
       "  0          0.541737\n",
       "  1          0.199687\n",
       "  2          0.036432\n",
       "  3          0.090279\n",
       "  4          0.390769\n",
       "               ...   \n",
       "  1926166    0.200385\n",
       "  1926167    0.072087\n",
       "  1926168    0.200385\n",
       "  1926169    0.162837\n",
       "  1926170    0.010894\n",
       "  Name: rating_scaled, Length: 1926171, dtype: float64)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff502c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = datasets['train'][0]\n",
    "X_test = datasets['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb310d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_users = pd.concat([X_train[\"userId\"], X_test[\"userId\"]])\n",
    "user_count = combined_users.nunique()\n",
    "\n",
    "combined_movie = pd.concat([X_train[\"movieId\"], X_test[\"movieId\"]])\n",
    "movie_count = combined_movie.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cc84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retnet_config = RetNetConfig(vocab_size = 200,\n",
    "                             decoder_layers=8,\n",
    "                             decoder_embed_dim=200,\n",
    "                             decoder_value_embed_dim=200,\n",
    "                             decoder_retention_heads=4,\n",
    "                             decoder_ffn_embed_dim=200,\n",
    "                             chunkwise_recurrent = False\n",
    "                                 )\n",
    "\n",
    "retnet_model = RetNetDecoder(retnet_config)\n",
    "batch_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e01b184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='sum'):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=reduction)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb5288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3292b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filteringRetNet import DatasetBatchIterator\n",
    "from filteringRetNet import NeuralColabFilteringRetNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb14073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop control parameters\n",
    "max_epochs = 1\n",
    "early_stop_epoch_threshold = 3\n",
    "no_loss_reduction_epoch_counter = 0\n",
    "min_loss = np.inf\n",
    "min_loss_model_weights = None\n",
    "history = []\n",
    "\n",
    "ncf_retnet = NeuralColabFilteringRetNet(user_count, \n",
    "                                        movie_count, \n",
    "                                        retnet_model,\n",
    "                                        hidden_size = retnet_config.decoder_embed_dim,\n",
    "                                        device = device).to(device)\n",
    "\n",
    "\n",
    "\n",
    "loss_criterion = RMSELoss(reduction='sum').to(device)\n",
    "#loss_criterion_2 = nn.L1Loss(reduction='sum').to(device)\n",
    "optimizer = optim.Adam(ncf_retnet.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39900d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bb0ac22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train phase:   1%|â–Ž                                                                  | 2/385 [01:13<3:55:17, 36.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#loss_2 = loss_criterion_2(outputs, y_batch)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m---> 28\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_start_time = time.perf_counter()\n",
    "for epoch in range(max_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': max_epochs}\n",
    "    epoch_start_time = time.perf_counter()\n",
    "\n",
    "    # Every epoch runs training on train set, followed by eval on test set\n",
    "    for phase in ('train', 'test'):\n",
    "        is_training = phase == 'train'\n",
    "        ncf_retnet.train(is_training)\n",
    "        running_loss = 0.0\n",
    "        #running_loss_2 = 0.0\n",
    "        n_batches = 0\n",
    "        total_batches = len(datasets[phase][0]) // batch_size\n",
    "        # Iterate on train/test datasets in batches\n",
    "        for x_batch, y_batch in  tqdm(DatasetBatchIterator(datasets[phase][0], datasets[phase][1], batch_size=batch_size, shuffle=is_training), desc=f'{phase} phase', total=total_batches):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # We zero out the loss gradient, since PyTorch by default accumulates gradients  \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # We need to compute gradients only during training\n",
    "            with torch.set_grad_enabled(is_training):\n",
    "                \n",
    "                outputs = ncf_retnet(x_batch[:, 0], x_batch[:, 1], )\n",
    "                loss = loss_criterion(outputs, y_batch)\n",
    "                #loss_2 = loss_criterion_2(outputs, y_batch)\n",
    "                if is_training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            #running_loss_2 += loss_2.item()\n",
    "        \n",
    "        # Compute overall epoch loss and update history tracker\n",
    "        epoch_loss = running_loss / len(datasets[phase][0])\n",
    "        stats[phase] = epoch_loss\n",
    "        #epoch_loss_2 = running_loss_2 / len(datasets[phase][0])\n",
    "                \n",
    "        history.append(stats)\n",
    "        \n",
    "\n",
    "        # Handle early stopping\n",
    "        if phase == 'test':\n",
    "            stats['time'] = time.perf_counter() - epoch_start_time\n",
    "            print('Epoch [{epoch:03d}/{total:03d}][Time:{time:.2f} sec] Train Loss: {train:.4f} / Validation Loss: {test:.4f}'.format(**stats))\n",
    "            if epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                min_loss_model_weights = copy.deepcopy(ncf_retnet.state_dict())\n",
    "                no_loss_reduction_epoch_counter = 0\n",
    "                min_epoch_number = epoch + 1\n",
    "            else:\n",
    "                no_loss_reduction_epoch_counter += 1\n",
    "    if no_loss_reduction_epoch_counter >= early_stop_epoch_threshold:\n",
    "        print(f'Early stopping applied. Minimal epoch: {min_epoch_number}')\n",
    "        break\n",
    "\n",
    "print(f'Training completion duration: {(time.perf_counter() - training_start_time):.2f} sec. Validation Loss: {min_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46bf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
